{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition\n",
    "\n",
    "#### Samir Valiyev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we are going to evaluate the performance of several models in classifiying human activity using data recorded with mobile sensors. Despite there are quite a big number of sensors built in smartphones we focused on __'Linear Accelerometer', 'Accelerometer X', 'Accelerometer Y', 'Accelerometer Z','Barometer'__ becuase they are common for a wide range of smartphone models and using the data recorded with this set of sensors is sufficient to distinguish the basic human activities. Data was recorded using Google Science Journal application.\n",
    "\n",
    "The activity types researched in this project are the following:\n",
    "__Going downstairs, going upstairs, running, sitting, walking__\n",
    "\n",
    "Each of the member of our team placed the smartphone with active sensors in the pelvic area and then performed one of the listed activity for a recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[Data_Preprocessing](#Data_Preprocessing)\n",
    "\n",
    "[Data_Visualization](#Data_Visualization)\n",
    "\n",
    "[Feature_selection](#Feature_selection)\n",
    "\n",
    "[Parameter_tuning](#Parameter_tuning)\n",
    "\n",
    "[Modelling_and_prediction](#Modelling_and_prediction)\n",
    "\n",
    "[Results](#Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data_Preprocessing'> Data_Preprocessing</a>\n",
    "\n",
    "The data obtained from sensors is not clean mainly because of the recording ability of the sensors. Senosrs record data in specific time intervals which depends on sensor type and model of the smartphone. So we imputed all the missing values using interpolation. Then we devided our data to chunks in the size of 2.5 seconds with 50% overlapping. Also we applied fast fourier transformation to the chunk set. \n",
    "\n",
    "The following step was to generate a set of features. We decided to extract the following features for each sensor:\n",
    "\n",
    "__mean, max, min, var, Q1, Q3, slope__\n",
    "\n",
    "__Slope__ is a feature which show the slope of the obtained data. We considered that it could help the model to distinguish upstairs and downstairs activities. At last all the computed data were concateneted to one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]\n",
    "list_csv = find_csv_filenames('Data', suffix='.csv') # List of csv files in our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(X):\n",
    "    return((X[-1]-X[0])/2500) # this function roughly determines the slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft(X):\n",
    "    return np.abs(np.fft.fft(pd.Series(np.around(X.astype(np.double),2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data preprocessing\n",
    "def datareader(file_dir, name):\n",
    "    df = pd.read_csv(file_dir) # create dataframe\n",
    "    sensors = ['timestamp','LinearAccelerometerSensor', 'AccX', 'AccY', 'AccZ','BarometerSensor'] # define list of sensors used\n",
    "    df = df[sensors].copy() # keep only the sensors defined above\n",
    "    df['timestamp'] = df['timestamp'] - df['timestamp'].min() # transform the time\n",
    "    last = df['timestamp'].max() # the last moment of activity\n",
    "    key = np.array(range(0,last+1)) # create the complete timeline\n",
    "    key = pd.DataFrame(key)\n",
    "    df = pd.merge(right=df, left=key, how='left', left_on=0, right_on='timestamp') #expand our dataset to the whole timelines\n",
    "    df = df[sensors[1:]] # drop the redundant timeline column\n",
    "    df = df.interpolate() # interpolate the the data in order to fill the missing values\n",
    "    label = np.empty([len(df.AccX),1], dtype='S10') # creating lable column\n",
    "    label[:,:] = name # adding the label\n",
    "    df['label'] = label # adding the labels for the dataframe\n",
    "    # chunk the data by 2500 milliseconds with 1250 ms overlap (2.5 seconds with 1.25s overlap)\n",
    "    gen = TimeseriesGenerator(df.values[:,0:-1], df.label.values, length=2500, stride=1250, batch_size=100000000000)\n",
    "    X, Y = gen[0] # extract the values from generator\n",
    "    # list of feature names\n",
    "    cols=[i+j for i in ['Lacc_','AccX_','AccY_','AccZ_','Bar_'] \n",
    "                for j in ['mean', 'max', 'min', 'var', 'Q1', 'Q3', 'slope']]\n",
    "    # create the feature set\n",
    "    X_data = pd.DataFrame(np.concatenate((np.mean(X, axis=1),\n",
    "                                          np.max(X, axis=1),\n",
    "                                          np.min(X, axis=1),\n",
    "                                          np.var(X, axis=1),\n",
    "                                          np.percentile(X, 25, axis=1),\n",
    "                                          np.percentile(X, 75, axis=1),\n",
    "                                          np.apply_along_axis(slope, axis=1, arr = X)), axis=1), columns=cols)\n",
    "    X_f = np.apply_along_axis(fft, axis=1, arr=X)\n",
    "    \n",
    "    cols_f=[i+j for i in ['Lacc_f_','AccX_f_','AccY_f_','AccZ_f_','Bar_f_'] \n",
    "                  for j in ['mean', 'max', 'min', 'var', 'Q1', 'Q3', 'slope']]\n",
    "    # create the feature set\n",
    "    X_f_data = pd.DataFrame(np.concatenate((np.mean(X_f, axis=1),\n",
    "                                          np.max(X_f, axis=1),\n",
    "                                          np.min(X_f, axis=1),\n",
    "                                          np.var(X_f, axis=1),\n",
    "                                          np.percentile(X, 25, axis=1),\n",
    "                                          np.percentile(X, 75, axis=1),\n",
    "                                          np.apply_along_axis(slope, axis=1, arr = X_f)), axis=1), columns=cols_f)\n",
    "    X_data = pd.concat([X_data,X_f_data], axis = 1)\n",
    "    Y_data = pd.DataFrame(Y) # predicting variable (the type of activity)\n",
    "    Data = pd.concat([X_data, Y_data], axis=1).dropna() # drop the rest of missing values   \n",
    "    return(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading csv files, extracting the features and concatinating them into dataframe\n",
    "for i,file_path in enumerate(list_csv):\n",
    "    file_dir = 'Data/' + file_path\n",
    "    label = file_dir.split('/')[1].split(' ')[0]\n",
    "    if i == 0:\n",
    "        df = datareader(file_dir, label)\n",
    "    else:\n",
    "        df = pd.concat([df, datareader(file_dir, label)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data_Visualization'> Data_Visualization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project we are going to visualize how our data is clustered. Because of that there are 5 dimensions in our dataset we choosed PCA method to visualize it. PCA is effected by scale so we need to scale the features in the data before applying PCA.The explained variance tells us how much information (variance) can be attributed to each of the principal components. So from the screeplot we can conclude that the major part of the data is attributed to the first principal component. We choosed PC_1 and PC_2 to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(train, test):\n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(train)\n",
    "    return enc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the types of the columns to the numeric one\n",
    "X_data = df.iloc[:,0:-1].apply(pd.to_numeric, errors='ignore')#.values\n",
    "Y_data = df.iloc[:,-1].apply(pd.to_numeric, errors='ignore')#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaled_data = preprocessing.scale(X_data.values)\n",
    " \n",
    "pca = PCA() # create a PCA object\n",
    "pca.fit(scaled_data) # do the math\n",
    "pca_data = pca.transform(scaled_data) # get PCA coordinates for scaled_data\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "per_var = per_var[per_var > 0]\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance',fontsize=10)\n",
    "plt.xlabel('Principal Component',fontsize=10)\n",
    "plt.title('Scree Plot',fontsize=10)\n",
    "fig.show()\n",
    "#fig.savefig('ScreePlot.png')\n",
    "col = encoder(Y_data, Y_data)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "for i in range(0,5):\n",
    "    plt.scatter(pca_data[col==i,0], pca_data[col==i,1], alpha=0.9)\n",
    "\n",
    "plt.legend(labels=['DownStairs', 'Run', 'Sitting', 'UpStairs', 'Walk'],fontsize=10)\n",
    "plt.xlabel('PCA1',fontsize=10)\n",
    "plt.ylabel('PCA2',fontsize=10)\n",
    "plt.title('PCA',fontsize=10)\n",
    "fig.show()\n",
    "#fig.savefig('PCA.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen on the plot above most of the points is for the Running activity. Other activities have relatively equal number of points. However the data is not clusterized. Below is printed the rank of 10 most important features according to their loading scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lacc_mean       0.164418\n",
      "Lacc_f_Q3       0.164418\n",
      "Bar_f_min       0.164096\n",
      "AccZ_f_Q1       0.163630\n",
      "AccZ_Q1         0.163630\n",
      "Lacc_f_min      0.161670\n",
      "Lacc_Q3         0.161518\n",
      "AccY_f_slope    0.160563\n",
      "AccY_slope      0.160563\n",
      "AccX_Q3         0.160445\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "loading_scores = pd.Series(pca.components_[0], index=list(X_data))\n",
    "sorted_loading_scores = (loading_scores.abs()).sort_values(ascending=False)\n",
    "top = sorted_loading_scores[0:10].index.values\n",
    "print(sorted_loading_scores[top])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the preprocessed data into train and test set with rate 70%\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, train_size = .7)\n",
    "Y_train_enc = encoder(Y_train, Y_train)\n",
    "Y_test_enc = encoder(Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Feature_selection'> Feature_selection</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection were provided using built in algorithm of Random Forest Classifier. After obtaining the importance score for each feature we select those of them which score was above 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Feature importance rank\n",
    "rffs = RandomForestClassifier()\n",
    "rffs.fit(X_train, Y_train_enc)\n",
    "imp = pd.Series(rffs.feature_importances_, X_train.columns).sort_values(ascending=False)\n",
    "(imp[imp>0.01]).plot(kind='bar', figsize=(15,5), fontsize = 10)\n",
    "plt.ylabel('Feature Importance Score', fontsize = 15)\n",
    "plt.title('Importance of Features', fontsize = 15)\n",
    "#fig = plt.gcf()\n",
    "#fig.savefig('FeaturImportance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rffs_selection = list(imp[imp > 0.01].index)\n",
    "X_train_rffs = X_train[rffs_selection]\n",
    "X_test_rffs = X_test[rffs_selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Parameter_tuning'> Parameter_tuning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train three models on our data __RandomForestClassifier(), GradientBoostingClassifier(), MLPClassifier()__ . First of all we need to define the optimal set of parameters for each model. Using Scikit-Learn’s RandomizedSearchCV method, we can define a grid of hyperparameter ranges, and randomly sample from the grid, performing K-Fold CV with each combination of values. We are computing the best parameters for the models trained both on the full set of features and on the reduced feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing parameter tuning for Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set a random grid\n",
      "Train model on a full feature set\n",
      "Train model on a rffs feature set\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Set a random grid')\n",
    "random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 500, num = 4)],# Number of trees in random forest\n",
    "               'max_features': ['auto', 'sqrt'], # Number of features to consider at every split\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 50, num = 4)], # Maximum number of levels in tree\n",
    "               'min_samples_split': [2, 4, 6], # Minimum number of samples required to split a node\n",
    "               'min_samples_leaf': [1, 2, 4], # Minimum number of samples required at each leaf node\n",
    "               'bootstrap': [True, False] # Method of selecting samples for training each tree\n",
    "              }\n",
    "\n",
    "# tuning the parameters\n",
    "rf_random_full = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, \n",
    "                                               n_iter = 5, cv = 3, random_state=42, n_jobs = -1)\n",
    "\n",
    "rf_random_rffs = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, \n",
    "                                               n_iter = 5, cv = 3, random_state=42, n_jobs = -1)\n",
    "print('Train model on a full feature set')\n",
    "rf_random_full.fit(X_train.values, Y_train_enc)\n",
    "print('Train model on a rffs feature set')\n",
    "rf_random_rffs.fit(X_train_rffs.values, Y_train_enc)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for the full feature set:\n",
      "\n",
      "{   'bootstrap': False,\n",
      "    'max_depth': 36,\n",
      "    'max_features': 'auto',\n",
      "    'min_samples_leaf': 1,\n",
      "    'min_samples_split': 2,\n",
      "    'n_estimators': 500}\n",
      "Best parameters for the rffs feature set:\n",
      "\n",
      "{   'bootstrap': False,\n",
      "    'max_depth': 36,\n",
      "    'max_features': 'auto',\n",
      "    'min_samples_leaf': 1,\n",
      "    'min_samples_split': 2,\n",
      "    'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters for the full feature set:\\n')\n",
    "pp.pprint(rf_random_full.best_params_)\n",
    "print('Best parameters for the rffs feature set:\\n')\n",
    "pp.pprint(rf_random_rffs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing parameter tuning for Gradient Boosting Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set a random grid\n",
      "Train model on a full feature set\n",
      "Train model on a rffs feature set\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Set a random grid')\n",
    "random_grid = {\n",
    "               'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 500, num = 4)], # Number of trees in random forest\n",
    "               'max_features': ['auto', 'sqrt'], # Number of features to consider at every split\n",
    "               'max_depth': [int(x) for x in np.linspace(1, 10, num = 4)], # Maximum number of levels in tree\n",
    "               'min_samples_split': [2, 4], # Minimum number of samples required to split a node\n",
    "               'min_samples_leaf': [1, 2], # Minimum number of samples required at each leaf node\n",
    "            }\n",
    "\n",
    "gb_random_full = RandomizedSearchCV(estimator=GradientBoostingClassifier(), param_distributions=random_grid,\n",
    "                                               n_iter=5, cv=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "gb_random_rffs = RandomizedSearchCV(estimator=GradientBoostingClassifier(), param_distributions=random_grid,\n",
    "                                               n_iter=5, cv=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "print('Train model on a full feature set')\n",
    "gb_random_full.fit(X_train.values, Y_train_enc)\n",
    "print('Train model on a rffs feature set')\n",
    "gb_random_rffs.fit(X_train_rffs.values, Y_train_enc)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for the full feature set:\n",
      "\n",
      "{   'max_depth': 4,\n",
      "    'max_features': 'sqrt',\n",
      "    'min_samples_leaf': 1,\n",
      "    'min_samples_split': 4,\n",
      "    'n_estimators': 500}\n",
      "Best parameters for the rffs feature set:\n",
      "\n",
      "{   'max_depth': 4,\n",
      "    'max_features': 'sqrt',\n",
      "    'min_samples_leaf': 1,\n",
      "    'min_samples_split': 4,\n",
      "    'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters for the full feature set:\\n')\n",
    "pp.pprint(gb_random_full.best_params_)\n",
    "print('Best parameters for the rffs feature set:\\n')\n",
    "pp.pprint(gb_random_rffs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing parameter tuning for MLP Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to scale the predictors before using it in MLP Classifier\n",
    "scaler = MinMaxScaler() # define scaler\n",
    "scaler.fit(X_train)\n",
    "X_tr_t = scaler.transform(X_train) # scale the training set\n",
    "X_ts_t = scaler.transform(X_test) # scale the test set\n",
    "scaler.fit(X_train_rffs)\n",
    "X_tr_t_rffs = scaler.transform(X_train_rffs) # scale the training set\n",
    "X_ts_t_rffs = scaler.transform(X_test_rffs) # scale the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set a random grid\n",
      "Train model on a full feature set\n",
      "Train model on a rffs feature set\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Set a random grid')\n",
    "random_grid = {  \n",
    "                'solver' : ['lbfgs', 'sgd', 'adam'],#The solver for weight optimization.\n",
    "                                    #‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    "                                    #‘sgd’ refers to stochastic gradient descent.\n",
    "                                    #‘adam’ refers to a stochastic gradient-based optimizer\n",
    "                'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ],#Maximum number of iterations. \n",
    "                                                            #The solver iterates until convergence (determined by ‘tol’) or \n",
    "                                                            #this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), \n",
    "                                                            #note that this determines the number of epochs (how many times each \n",
    "                                                            #data point will be used), not the number of gradient steps. \n",
    "                'alpha': 10.0 ** -np.arange(1, 10), #L2 penalty (regularization term) parameter.\n",
    "                'hidden_layer_sizes':[(50,50,50,50), (100,100,100,100)] #The ith element represents the number of neurons in the ith hidden layer.\n",
    "            }\n",
    "\n",
    "mlp_random_full = RandomizedSearchCV(estimator= MLPClassifier(verbose=False), param_distributions=random_grid,\n",
    "                                               n_iter=5, cv=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "mlp_random_rffs = RandomizedSearchCV(estimator= MLPClassifier(verbose=False), param_distributions=random_grid,\n",
    "                                               n_iter=5, cv=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "print('Train model on a full feature set')\n",
    "mlp_random_full.fit(X_tr_t, Y_train_enc)\n",
    "print('Train model on a rffs feature set')\n",
    "mlp_random_rffs.fit(X_train_rffs, Y_train_enc)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for the full feature set:\n",
      "\n",
      "{   'alpha': 1e-07,\n",
      "    'hidden_layer_sizes': (100, 100, 100, 100),\n",
      "    'max_iter': 1200,\n",
      "    'solver': 'lbfgs'}\n",
      "Best parameters for the rffs feature set:\n",
      "\n",
      "{   'alpha': 0.01,\n",
      "    'hidden_layer_sizes': (100, 100, 100, 100),\n",
      "    'max_iter': 1100,\n",
      "    'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters for the full feature set:\\n')\n",
    "pp.pprint(mlp_random_full.best_params_)\n",
    "print('Best parameters for the rffs feature set:\\n')\n",
    "pp.pprint(mlp_random_rffs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Modelling_and_prediction'> Modelling_and_prediction</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the optimal parameters for each model we are going to train the models using k_fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(model, X_data, Y_data, scale = False):        \n",
    "    kf = KFold(n_splits = 5, shuffle=True)\n",
    "    Y_data_enc = encoder(Y_data, Y_data)\n",
    "    i = 0\n",
    "    for tr_ind,te_ind in kf.split(X_data.values):\n",
    "        if scale == True : \n",
    "            scaler = MinMaxScaler() # define scaler\n",
    "            scaler.fit(X_data.values[tr_ind,:])\n",
    "            X_train = scaler.transform(X_data.values[tr_ind,:])   # scale the training set\n",
    "            X_test = scaler.transform(X_data.values[te_ind,:])   # scale the test set\n",
    "        else:\n",
    "            X_train = X_data.values[tr_ind,:]\n",
    "            X_test = X_data.values[te_ind,:]\n",
    "            \n",
    "        model.fit(X_train, Y_data_enc[tr_ind])\n",
    "        if i == 0:\n",
    "            Y_pred = model.predict(X_test)\n",
    "            Y_true = Y_data_enc[te_ind]\n",
    "            i +=1\n",
    "        else:\n",
    "            Y_pred = np.concatenate((Y_pred, model.predict(X_data.values[te_ind,:])))\n",
    "            Y_true = np.concatenate((Y_true,  Y_data_enc[te_ind]))\n",
    "            i +=1\n",
    "    return {'True':Y_true, 'Prediction':Y_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(output):\n",
    "    activity = ['DownStairs', 'Run', 'Sitting', 'UpStairs', 'Walk']\n",
    "    print(pd.DataFrame(confusion_matrix(output['True'], output['Prediction']), index = activity, columns = activity))\n",
    "    print(classification_report(output['True'], output['Prediction'],target_names=activity))\n",
    "    print(\"accuracy_score: \", accuracy_score(output['True'], output['Prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_full = RandomForestClassifier(**rf_random_full.best_params_)\n",
    "rf_rffs = RandomForestClassifier(**rf_random_rffs.best_params_)\n",
    "output_rf_full = k_fold(rf_full, X_data, Y_data)\n",
    "output_rf_rffs = k_fold(rf_rffs, X_data, Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DownStairs   Run  Sitting  UpStairs  Walk\n",
      "DownStairs         394     0        0         2     0\n",
      "Run                  0  4249        0         0     1\n",
      "Sitting              2     0     9211         2     3\n",
      "UpStairs             3     0        0       336     0\n",
      "Walk                 0     3        0         0  1715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  DownStairs       0.99      0.99      0.99       396\n",
      "         Run       1.00      1.00      1.00      4250\n",
      "     Sitting       1.00      1.00      1.00      9218\n",
      "    UpStairs       0.99      0.99      0.99       339\n",
      "        Walk       1.00      1.00      1.00      1718\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     15921\n",
      "   macro avg       0.99      1.00      1.00     15921\n",
      "weighted avg       1.00      1.00      1.00     15921\n",
      "\n",
      "accuracy_score:  0.9989950380001256\n"
     ]
    }
   ],
   "source": [
    "prediction(output_rf_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DownStairs   Run  Sitting  UpStairs  Walk\n",
      "DownStairs         393     0        0         3     0\n",
      "Run                  0  4248        0         0     2\n",
      "Sitting              2     0     9210         2     4\n",
      "UpStairs             3     0        0       336     0\n",
      "Walk                 0     5        0         0  1713\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  DownStairs       0.99      0.99      0.99       396\n",
      "         Run       1.00      1.00      1.00      4250\n",
      "     Sitting       1.00      1.00      1.00      9218\n",
      "    UpStairs       0.99      0.99      0.99       339\n",
      "        Walk       1.00      1.00      1.00      1718\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     15921\n",
      "   macro avg       0.99      1.00      0.99     15921\n",
      "weighted avg       1.00      1.00      1.00     15921\n",
      "\n",
      "accuracy_score:  0.9986809873751649\n"
     ]
    }
   ],
   "source": [
    "prediction(output_rf_rffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_full = GradientBoostingClassifier(**gb_random_full.best_params_)\n",
    "gb_rffs = GradientBoostingClassifier(**gb_random_rffs.best_params_)\n",
    "output_gb_full = k_fold(gb_full, X_data, Y_data)\n",
    "output_gb_rffs = k_fold(gb_rffs, X_data, Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DownStairs   Run  Sitting  UpStairs  Walk\n",
      "DownStairs         388     1        0         7     0\n",
      "Run                  0  4249        0         0     1\n",
      "Sitting              1     0     9210         3     4\n",
      "UpStairs             8     0        0       331     0\n",
      "Walk                 0     0        0         0  1718\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  DownStairs       0.98      0.98      0.98       396\n",
      "         Run       1.00      1.00      1.00      4250\n",
      "     Sitting       1.00      1.00      1.00      9218\n",
      "    UpStairs       0.97      0.98      0.97       339\n",
      "        Walk       1.00      1.00      1.00      1718\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     15921\n",
      "   macro avg       0.99      0.99      0.99     15921\n",
      "weighted avg       1.00      1.00      1.00     15921\n",
      "\n",
      "accuracy_score:  0.9984297468751963\n"
     ]
    }
   ],
   "source": [
    "prediction(output_gb_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DownStairs   Run  Sitting  UpStairs  Walk\n",
      "DownStairs         392     0        0         4     0\n",
      "Run                  0  4249        0         0     1\n",
      "Sitting              4     0     9208         2     4\n",
      "UpStairs             8     0        1       330     0\n",
      "Walk                 0     0        0         0  1718\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  DownStairs       0.97      0.99      0.98       396\n",
      "         Run       1.00      1.00      1.00      4250\n",
      "     Sitting       1.00      1.00      1.00      9218\n",
      "    UpStairs       0.98      0.97      0.98       339\n",
      "        Walk       1.00      1.00      1.00      1718\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     15921\n",
      "   macro avg       0.99      0.99      0.99     15921\n",
      "weighted avg       1.00      1.00      1.00     15921\n",
      "\n",
      "accuracy_score:  0.9984925570001885\n"
     ]
    }
   ],
   "source": [
    "prediction(output_gb_rffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_full = MLPClassifier(**mlp_random_full.best_params_)\n",
    "mlp_rffs = MLPClassifier(**mlp_random_rffs.best_params_)\n",
    "output_mlp_full = k_fold(mlp_full, X_data, Y_data)\n",
    "output_mlp_rffs = k_fold(mlp_rffs, X_data, Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(output_mlp_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DownStairs   Run  Sitting  UpStairs  Walk\n",
      "DownStairs           0    81        0       158   157\n",
      "Run                  0   823        0      1694  1733\n",
      "Sitting              0  1864        0      3662  3692\n",
      "UpStairs             0    69        0       148   122\n",
      "Walk                 0   347        0       706   665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  DownStairs       0.00      0.00      0.00       396\n",
      "         Run       0.26      0.19      0.22      4250\n",
      "     Sitting       0.00      0.00      0.00      9218\n",
      "    UpStairs       0.02      0.44      0.04       339\n",
      "        Walk       0.10      0.39      0.16      1718\n",
      "\n",
      "   micro avg       0.10      0.10      0.10     15921\n",
      "   macro avg       0.08      0.20      0.09     15921\n",
      "weighted avg       0.08      0.10      0.08     15921\n",
      "\n",
      "accuracy_score:  0.10275736448715533\n"
     ]
    }
   ],
   "source": [
    "prediction(output_mlp_rffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Results'> Results</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the predictive abilities of Random Forest and Gradient Boosting Classifiers are very strong. The accuracy score for these models is near 99%. From confusion matrix it can be seen that the minor number of mistakes is made while distinguishing Downstairs and Upstairs activities. These models give a good results even on a reduced set of selected features. So in order to optimize the computation we can train model on the smaller feature set. MLP Classifier failed to recognize activities.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "392.8px",
    "left": "1124.8px",
    "right": "20px",
    "top": "124px",
    "width": "328.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
